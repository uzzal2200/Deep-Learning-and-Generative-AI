{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "m3bXiIVZL686"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "- Transfer Learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second, related task.\n",
        "- Instead of training a model from scratch, which requires a large amount of labeled data and computational resources, transfer learning allows us to take advantage of knowledge gained from a pre-trained model on a large dataset (like ImageNet) and fine-tune it for a new, specific task."
      ],
      "metadata": {
        "id": "fQzi6K3iFfhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Concepts in Transfer Learning:\n",
        "\n",
        "* Pre-Trained Models:\n",
        " - These models are trained on a large dataset (e.g., ImageNet) for a generic task like image classification. The learned features (e.g., edges, shapes) are transferable to different but related tasks.\n",
        "* Fine-Tuning:\n",
        " - Involves adjusting the pre-trained modelâ€™s weights on a new dataset, where the earlier layers (closer to the input) remain largely unchanged and the later layers are updated to adapt to the new task.\n",
        "* Feature Extraction:\n",
        " -  A technique where only the final layers of the pre-trained model are replaced with new layers suited for the specific task, while the rest of the layers are frozen to avoid updating their weights."
      ],
      "metadata": {
        "id": "EUimW_yYFojR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Use Transfer Learning?\n",
        "* Faster Training:\n",
        " - Significantly reduces training time since a large portion of the model is already pre-trained.\n",
        "* Less Data Requirement:\n",
        " - It is effective when you have a small dataset because the pre-trained model already has learned useful features.\n",
        "* Better Performance:\n",
        " - Pre-trained models are often more generalizable, especially for complex tasks, because they are trained on vast and diverse datasets."
      ],
      "metadata": {
        "id": "foxU0wluFkWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XBAKxAZsFRYu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "prleyiyRHIAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data augmentation and normalization for training\n",
        "transform_train = T.Compose([\n",
        "    T.Resize(256),  # Resize images to 256x256\n",
        "    T.RandomResizedCrop(227),  # Random crop to 224x224\n",
        "    T.RandomHorizontalFlip(),  # Data Augmentation (Horizontal Flip)\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Test/Validation transformation\n",
        "transform_test = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(227),  # Center crop to 224x224\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset (Assuming Cats vs Dogs is in 'data/cats_vs_dogs')\n",
        "train_dataset = datasets.ImageFolder(root='cats_vs_dogs/train', transform=transform_train)\n",
        "test_dataset = datasets.ImageFolder(root='cats_vs_dogs/test', transform=transform_test)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BpuB2tYG6P4",
        "outputId": "e3b72e29-d85c-48d9-d3ef-e8668cae7d28"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "OSlWfpZ1HDbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.alexnet(pretrained=True)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "O000GKWoLZIx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTKUywDQORFz",
        "outputId": "194561ac-4316-450f-fc52-1e016b8b162d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.classifier[6] = nn.Linear(4096,2)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "g-KQrKtPOan7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer (using SGD with momentum)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "# Training the model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step() # update weights\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = outputs.max(1) # return max value and index\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()  # .item() is used to convert the result of .sum() into a regular Python integer.\n",
        "\n",
        "            # Print loss and accuracy for each batch\n",
        "            running_loss += loss.item()\n",
        "            # if i % 10 == 9:  # print every 100 mini-batches 99 so it does not print at 0 epoch\n",
        "            #     print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], '\n",
        "            #           f'Loss: {running_loss / 100:.4f}, Accuracy: {100.0 * correct / total:.2f}%')\n",
        "            #     running_loss = 0.0\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
        "                      f'Loss: {running_loss / 100:.4f}, Accuracy: {100.0 * correct / total:.2f}%')\n",
        "\n",
        "\n",
        "# Test the model\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100.0 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "id": "lPyVJZGAG15m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLmkjJ5pHA8x",
        "outputId": "95c943dd-8fe7-419b-f087-214080a54a3e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Loss: 0.0147, Accuracy: 94.00%\n",
            "Epoch [2/2], Loss: 0.0094, Accuracy: 98.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aK5hCtpHB5_",
        "outputId": "acafdaad-158e-4104-f766-04eeebba0d0a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.50%\n"
          ]
        }
      ]
    }
  ]
}