{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eosshrgDJJuG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdQxm2buOqRE"
      },
      "outputs": [],
      "source": [
        "# Data transformations for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)), # ResNet expects 224x224 images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229, 0.224, 0.225])  # Normalization for pre-trained models\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WEtrk-wPkxL",
        "outputId": "a02597b5-ac65-4ee7-888f-b5598ece438e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Loading CIFAR-10 datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuvWPHHbQIzr"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg9wdFLLQ2zq",
        "outputId": "582a3efe-1293-412d-b61c-01870b950e78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 95.1MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1024\n",
            "Linear(in_features=1024, out_features=10, bias=True)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "# print(model.parameters())\n",
        "# Freeze the early layers (feature extractor)\n",
        "for param in model.parameters():\n",
        "  # print(param)\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Replace the classifier layer (fully connected layer)\n",
        "num_ftrs = model.classifier.in_features\n",
        "print(num_ftrs)\n",
        "model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "print(model.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af4hN8CzW1VY"
      },
      "outputs": [],
      "source": [
        "# move Model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Provide the input_size argument to the summary function\n",
        "summary(model,(3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4GRqpqCdfrk",
        "outputId": "bd9c5fb8-31bb-4538-a242-f45bc34184aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]\n",
            "Training    - Loss: 0.8489, Accuracy: 74.04%\n",
            "Validation  - Loss: 0.6140, Accuracy: 80.03%\n",
            "\n",
            "Epoch [2/5]\n",
            "Training    - Loss: 0.5502, Accuracy: 81.84%\n",
            "Validation  - Loss: 0.5434, Accuracy: 81.42%\n",
            "\n",
            "Epoch [3/5]\n",
            "Training    - Loss: 0.5087, Accuracy: 82.89%\n",
            "Validation  - Loss: 0.5291, Accuracy: 82.26%\n",
            "\n",
            "Epoch [4/5]\n",
            "Training    - Loss: 0.4895, Accuracy: 83.26%\n",
            "Validation  - Loss: 0.5149, Accuracy: 82.31%\n",
            "\n",
            "Epoch [5/5]\n",
            "Training    - Loss: 0.4770, Accuracy: 83.65%\n",
            "Validation  - Loss: 0.5253, Accuracy: 81.98%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, test_loader, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_running_loss = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and accuracy calculation\n",
        "            total_running_loss += loss.item() * inputs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct_train += preds.eq(labels).sum().item()\n",
        "\n",
        "        # Average loss and accuracy for the training set\n",
        "        avg_loss = total_running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = correct_train / len(train_loader.dataset)\n",
        "        train_losses.append(avg_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_val_loss = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Loss and accuracy calculation\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct_val += preds.eq(labels).sum().item()\n",
        "\n",
        "        # Average loss and accuracy for the validation set\n",
        "        val_loss = running_val_loss / len(test_loader.dataset)\n",
        "        val_accuracy = correct_val / len(test_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
        "        print(f\"Training    - Loss: {avg_loss:.4f}, Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "        print(f\"Validation  - Loss: {val_loss:.4f}, Accuracy: {val_accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, test_loader, epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nb2qYHXHleW4",
        "outputId": "62308e5e-4802-4d76-af05-13515668dfbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss :  0.5253, Test Accuracy : 0.8198\n"
          ]
        }
      ],
      "source": [
        "# Evaluate function\n",
        "def evaluate_model(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for input, label in test_loader:\n",
        "      input, label = input.to(device), label.to(device)\n",
        "      outputs = model(input)\n",
        "      loss = criterion(outputs, label)\n",
        "      test_loss += loss.item() * input.size(0)\n",
        "      pred = outputs.argmax(dim=1)\n",
        "      correct += pred.eq(label).sum().item()\n",
        "  avg_loss = test_loss / len(test_loader.dataset)\n",
        "  accuracy = correct/  len(test_loader.dataset)\n",
        "  print(f'Test Loss : {avg_loss: .4f}, Test Accuracy : {accuracy:.4f}')\n",
        "\n",
        "evaluate_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}